### 1. *Introdução à Engenharia de Dados no Contexto Itaú*
- *Objetivo:* Entender o papel do engenheiro de dados e o ambiente do Itaú.
  - *Conteúdo:*
    - Visão geral do fluxo de dados no Itaú e introdução ao Datamesh.
    - Engenharia de Software X Engenharia de dados: diferenças e como uma complementa a outra.
    - Responsabilidades de um engenheiro de dados no contexto de uma instituição financeira.
  - *Atividade:* Discussão sobre como os dados são usados no Itaú para tomada de decisão e conformidade.

### 2. *SQL e Manipulação de Dados*
  - *Objetivo:* Fortalecer habilidades em SQL para manipulação e extração de dados.
  - *Conteúdo:*
    - Consultas SQL básicas e avançadas.
    - Operações de transformação e limpeza de dados com SQL.
    - Introdução ao DuckDB e como ele pode ser utilizado em pipelines de dados.
    - Integração do SQL com Python, DuckDB e PySpark.
    - Introdução ao Athena e como ele será aa principal ferramenta de consulta no Datamesh.
  - *Atividade:* Exercícios práticos de SQL com amostras de dados reais.

### 3. *Python para Engenharia de Dados*
- *Objetivo:* Ensinar Python aplicado ao desenvolvimento de pipelines de dados.
- *Conteúdo:*
  - Introdução a DataFrames com bibliotecas como Pandas, Polars, Dask e integração com Spark.
  - Boas práticas de programação e modularização de código em Python para ETL.
  - Testes unitários: como garantir a qualidade do código mesmo em pipelines de dados.
- *Atividade:* Criação de scripts Python para manipulação básica de dados e testes unitários.

### 4. *DataFrames e Manipulação de Dados em Spark*
  - *Objetivo:* Dominar DataFrames para processamento em larga escala.
  - *Conteúdo:*
    - Introdução ao Spark e seu funcionamento.
    - Introdução a DataFrames: conceitos e operações básicas.
    - Comparação entre DataFrames em Pandas e PySpark.
    - Manipulação de grandes volumes de dados com PySpark.
  - *Atividade:* Exercícios de manipulação de dados com DataFrames em PySpark.

### 5. *Processos de ETL usando AWS Glue e Lambda*
  - *Objetivo:* Ensinar o uso do AWS Glue para desenvolvimento e automação de ETL.
  - *Conteúdo:*
    - Arquitetura do AWS Glue e principais componentes.
    - Desenvolvimento de pipelines ETL com AWS Glue e PySpark.
    - Glue Data Quality para validação e monitoramento de dados.
    - Integração com AWS Lambda para automação de pipelines.
  - *Atividade:* Criação de um pipeline ETL completo no AWS Glue.

### 6. *Spark e PySpark para Processamento Distribuído*
  - *Objetivo:* Capacitar para o uso do Spark e PySpark em grandes volumes de dados.
  - *Conteúdo:*
    - Fundamentos do processamento distribuído com Spark.
    - Operações com PySpark, como filtragem, agregação, joins e particionamento.
    - Performance tuning e otimização de tarefas em Spark.
  - *Atividade:* Processamento e análise de dados de grande volume com PySpark.

### 7. *Arquitetura de Dados no Itaú: DataMesh e AWS*
  - *Objetivo:* Compreender a arquitetura de dados baseada em DataMesh e AWS.
  - *Conteúdo:*
    - Conceitos de DataMesh: governança e descentralização de dados.
    - Data Lake e Data Warehouse no AWS: S3, Glue Catalog e Athena.
    - Particionamento, versionamento e gerenciamento de dados no AWS.
  - *Atividade:* Exercício prático de modelagem de dados para um cenário do Itaú.

### 8. *Qualidade de Dados e LGPD*
  - *Objetivo:* Aplicar práticas de qualidade e segurança de dados.
  - *Conteúdo:*
    - Ferramentas de qualidade de dados (AWS Glue Data Quality).
    - Validação e conformidade com LGPD em pipelines de dados.
  - *Atividade:* Configuração de rotinas de validação de qualidade e conformidade em um pipeline.

### 9. *Projeto Final e Integração de Conceitos*
  - *Objetivo:* Aplicar todos os conceitos em um projeto realista.
  - *Conteúdo:*
    - Desenvolvimento de um pipeline end-to-end que integra todas as ferramentas e práticas aprendidas.
    - Documentação, monitoramento e ajuste do pipeline.
  - *Atividade:* Projeto de dados integrando Python, PySpark, Glue, e monitoramento de qualidade com Glue Data Quality.